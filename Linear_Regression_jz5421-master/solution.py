import numpy as np

N = 25
X = np.reshape(np.linspace(0, 0.9, N), (N, 1))
Y = np.cos(10 * X ** 2) + 0.1 * np.sin(100 * X)


class LinearRegression:
    def __init__(self, basis='polynomial', J=1):
        self.J = J
        self.basis = basis

        self.mle_w = None
        self.mle_variance = None

    def design_matrix(self, X):
        if self.basis == 'polynomial':
            return self._polynomial_design_matrix(X)
        else:
            return self._trigonometric_design_matrix(X)

    def _polynomial_design_matrix(self, X):
        """ Return polynomial design matrix of degree J with shape (N, M)

            Args:
                X: input vector of shape (N, 1)

            Output: polynomial design matrix of shape (N, M)
        """

        # --- ENTER SOLUTION HERE ---
        M = range(self.J + 1)  # 0, 1, ..., J
        Phi = np.ones(len(M))
        for i in range(0, len(X)):
            Phi = np.row_stack([Phi, np.power(X[i], M)])
        Phi = np.delete(Phi, 0, axis=0)
        return Phi

    def _trigonometric_design_matrix(self, X):
        """ Return trigonometric design matrix of degree J with shape (N, M)

            Args:
                X: input vector of shape (N, 1)

            Output: polynomial design matrix of shape (N, M)
        """

        # --- ENTER SOLUTION HERE ---
        Phi = np.ones(len(X))
        for i in range(1, self.J + 1):  # 2 * J + 1
            Phi = np.column_stack([Phi, np.sin(2 * np.pi * i * X)])
            Phi = np.column_stack([Phi, np.cos(2 * np.pi * i * X)])
        return Phi

    def fit(self, X, Y):
        """ Find maximum likelihood (MLE) solution, given basis Phi and output Y.

        Args:
            Phi: design matrix of shape (M, N)
            Y: vector of shape (N, 1)
            variance: scalar variance

        The function should not return anything, but instead
            1. save maximum likelihood for weights w, a numpy vector of shape (M, N), as variable 'self.mle_w'
            2. save maximum likelihood for variance as float as variable 'self.mle_variance'
        """

        Phi = self.design_matrix(X)

        # --- ENTER SOLUTION HERE ---
        if self.J != 0:
            self.mle_w = np.linalg.inv(Phi.T @ Phi) @ Phi.T @ Y
            self.mle_variance = np.mean((np.square(Phi @ self.mle_w - Y)))
        else:
            self.mle_w = 1 / (Phi.T @ Phi) * Phi.T @ Y
            self.mle_variance = ((Phi * self.mle_w - Y) ** 2) / N

    def predict(self, X_predict):
        """ Make a prediction using fitted solution.

        Args:
            X_predict: point to make prediction, vector of shape (V, 1)

        Output prediction as numpy vector of shape (V, 1)
        """

        # --- ENTER SOLUTION HERE ---
        # hint: remember that you can use functions like 'self.design_matrix(...)'
        #       and the fitted vector 'self.mle_w' here.
        Y_predict = self.design_matrix(X_predict) @ self.mle_w
        return Y_predict

    def predict_range(self, N_points, xmin, xmax):
        """ Make a prediction along a predefined range.

        Args:
            N_points: number of points to evaluate within range
            xmin: start of range to predict
            xmax: end of range to predict

        Returns a tuple containing:
            - numpy vector of shape (N_points, 1) for predicted X locations
            - numpy vector of shape (N_points, 1) for corresponding predicted values Y
        """

        # --- ENTER SOLUTION HERE ---
        X_predict = np.linspace(xmin, xmax, N_points)
        Y_predict = self.predict(X_predict)
        return X_predict, Y_predict


def leave_one_out_cross_validation(model, X, Y):
    """ Function to perform leave-one-out cross validation.
    
    Args:
        model: Model to perform leave-one-out cross validation.
        X: Full dataset X, of which different folds should be made.
        Y: Labels of dataset X

    Should return two floats:
        - the average test error over different folds
        - the average mle variance over different folds
    """
    N = len(X)

    # --- ENTER SOLUTION HERE ---
    # Hint: use the functions 'model.fit()' to fit on train folds and
    #       the function 'model.predict() to predict on test folds.
    test_error = []
    mle_variance = []
    for i in range(N):
        test_datax = X[i]
        test_datay = Y[i]
        train_datax = X[:]
        train_datay = Y[:]
        train_datax = np.delete(train_datax, i, axis=0)
        train_datay = np.delete(train_datay, i, axis=0)
        model.fit(train_datax, train_datay)
        test_error.append(np.square(test_datay - model.predict(test_datax)))
        mle_variance.append(model.mle_variance)
    average_test_error = np.mean(test_error)
    average_mle_variance = np.mean(mle_variance)
    return average_test_error, average_mle_variance

    # Descriptive Question:
    # An overfitted model is a statistical model that has more parameters than can
    # be justified by the data. It is too close or exact to the training data so it
    # may fail to fit additional data or predict future observations reliably. To
    # avoid overfitting, we can simplify the model by using cross validation, limiting
    # the number of parameters, and adding a penalty term to the loss function.
    # Furthermore, when iteratively training a learning algorithm, early stopping can be used.
